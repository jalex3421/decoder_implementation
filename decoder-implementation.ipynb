{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Implementing a GPT-Like Language Model from Scratch in PyTorch**\n\nAuthor: Alejandro Meza Tudela","metadata":{"id":"dED7z33VwTNh"}},{"cell_type":"markdown","source":"We are going to try to implement GPT architecture using such a good resource as:\n\n* **[Let's build GPT: from scratch, in code, spelled out]**  from Andrej Karpathy\n\nhttps://www.youtube.com/watch?v=kCc8FmEb1nY\n\nSpecifically, we are going to try to implement the decoder of the transfomer architecture.","metadata":{"id":"A9AKquBnxJhQ"}},{"cell_type":"markdown","source":"## 1. Preparations for the implementation","metadata":{"id":"idMw5rnPx82v"}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:09:46.973065Z","iopub.execute_input":"2025-05-31T06:09:46.973687Z","iopub.status.idle":"2025-05-31T06:09:51.095124Z","shell.execute_reply.started":"2025-05-31T06:09:46.973663Z","shell.execute_reply":"2025-05-31T06:09:51.094564Z"},"id":"MYR-SfInyZdr","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:01.115498Z","iopub.execute_input":"2025-05-31T06:10:01.116168Z","iopub.status.idle":"2025-05-31T06:10:01.953084Z","shell.execute_reply.started":"2025-05-31T06:10:01.116145Z","shell.execute_reply":"2025-05-31T06:10:01.952373Z"},"id":"vDRe70Z-wSKK","outputId":"51528dca-ea7a-4e97-c1aa-cdc1ce9f180a","trusted":true},"outputs":[{"name":"stdout","text":"--2025-05-31 06:10:01--  https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================>]   1.06M  5.70MB/s    in 0.2s    \n\n2025-05-31 06:10:01 (5.70 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"with open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:10:08.733182Z","iopub.execute_input":"2025-05-31T06:10:08.733702Z","iopub.status.idle":"2025-05-31T06:10:08.738695Z","shell.execute_reply.started":"2025-05-31T06:10:08.733681Z","shell.execute_reply":"2025-05-31T06:10:08.738146Z"},"id":"_NInkX2b_2bQ","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(f'length of the text: {len(text)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:11.159961Z","iopub.execute_input":"2025-05-31T06:10:11.160680Z","iopub.status.idle":"2025-05-31T06:10:11.165013Z","shell.execute_reply.started":"2025-05-31T06:10:11.160654Z","shell.execute_reply":"2025-05-31T06:10:11.164231Z"},"id":"110Zpxzl_4e-","outputId":"c3faa26d-0bfd-4ac4-e46a-e64c6546239b","trusted":true},"outputs":[{"name":"stdout","text":"length of the text: 1115394\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"#The first step is to get the unique characters that are in the dataset\nchars = sorted(list(set(text)))\nvocab_size = len(chars) #possible elements of our sequence\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:12.672085Z","iopub.execute_input":"2025-05-31T06:10:12.672358Z","iopub.status.idle":"2025-05-31T06:10:12.686537Z","shell.execute_reply.started":"2025-05-31T06:10:12.672337Z","shell.execute_reply":"2025-05-31T06:10:12.685810Z"},"id":"Kcrs7_I9_9S0","outputId":"4dda8f57-3acd-4ccb-f880-22876f2c40bd","trusted":true},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 2.Easy definition of encoder/decoder and train/val split","metadata":{"id":"l272wOwxy7Sy"}},{"cell_type":"code","source":"#define a way to map string to integers\nstring_to_integer = {ch:i for i,ch in enumerate(chars)}\ninteger_to_string = {i:ch for i,ch in enumerate(chars)}\n\n#define functions to encode and decode our strings\nencode = lambda s: [string_to_integer[c] for c in s]\ndecode = lambda l: ''.join([integer_to_string[i] for i in l])\n\nprint(encode('this is a just an example!'))\nprint(decode(encode('this is a just an example!')))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:26.209823Z","iopub.execute_input":"2025-05-31T06:10:26.210090Z","iopub.status.idle":"2025-05-31T06:10:26.215519Z","shell.execute_reply.started":"2025-05-31T06:10:26.210067Z","shell.execute_reply":"2025-05-31T06:10:26.214704Z"},"id":"TYWmry1KxeJS","outputId":"4d93f028-59cd-4c56-eb3d-4bf0a064fbb2","trusted":true},"outputs":[{"name":"stdout","text":"[58, 46, 47, 57, 1, 47, 57, 1, 39, 1, 48, 59, 57, 58, 1, 39, 52, 1, 43, 62, 39, 51, 54, 50, 43, 2]\nthis is a just an example!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Time to try to encode/decode the text that we read previously\n","metadata":{"id":"HvC1hZtuywhU"}},{"cell_type":"code","source":"#time to try to encode/decode the text that we read previously\ndata = torch.tensor(encode(text),dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:100])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:28.456917Z","iopub.execute_input":"2025-05-31T06:10:28.457416Z","iopub.status.idle":"2025-05-31T06:10:28.629295Z","shell.execute_reply.started":"2025-05-31T06:10:28.457395Z","shell.execute_reply":"2025-05-31T06:10:28.628356Z"},"id":"mO161uBExtRr","outputId":"81fd1f73-56f9-4d72-f595-b9e332a9c3cf","trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Let's define our 2 train/val sets.","metadata":{"id":"CiCxV20hy2ht"}},{"cell_type":"code","source":"threshold = int(0.9*len(data))\ntrain_data = data[:threshold]\nvalidation_data = data[threshold:]","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:10:34.063122Z","iopub.execute_input":"2025-05-31T06:10:34.063948Z","iopub.status.idle":"2025-05-31T06:10:34.068533Z","shell.execute_reply.started":"2025-05-31T06:10:34.063907Z","shell.execute_reply":"2025-05-31T06:10:34.067558Z"},"id":"yawVZ8K6zGA7","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Define the number of consecutive data elements to be processed at once\nblock_size = 8\ntrain_data[:block_size+1]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:36.903998Z","iopub.execute_input":"2025-05-31T06:10:36.904244Z","iopub.status.idle":"2025-05-31T06:10:36.910977Z","shell.execute_reply.started":"2025-05-31T06:10:36.904228Z","shell.execute_reply":"2025-05-31T06:10:36.910379Z"},"id":"zPd2rADszZBA","outputId":"4f59b956-eae0-4be2-c865-1982ecd8a8ee","trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## 3.Basic encoder logic and implementation of function to obtain batch","metadata":{"id":"YNlJyL4f1EXo"}},{"cell_type":"markdown","source":"Since in a basic encoder definition, the block X mission is to predict the block X+1, let's see visually what is the meaning of this.","metadata":{"id":"_SSVpP5oz2cX"}},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f'when input is {context} the target: {target}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:39.113586Z","iopub.execute_input":"2025-05-31T06:10:39.113867Z","iopub.status.idle":"2025-05-31T06:10:39.122358Z","shell.execute_reply.started":"2025-05-31T06:10:39.113848Z","shell.execute_reply":"2025-05-31T06:10:39.121725Z"},"id":"zJZgVChI0BWb","outputId":"d5811bc8-4642-40f9-9747-55dd659e9aa0","trusted":true},"outputs":[{"name":"stdout","text":"when input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Now, it's time to define the way to obtain a batch to train our transformer model.","metadata":{"id":"DHt3Wt_d1Qmz"}},{"cell_type":"code","source":"torch.manual_seed(1337) #for reproducibility\nbatch_size = 4 #groups to be process in parallel\nblock_size = 8 #context window lenght\n\ndef get_batch(split,batch_size,block_size):\n  data = train_data if split=='train' else validation_data\n  #define the index to obtain the information\n  ix = torch.randint(len(data) - block_size, (batch_size,))\n  #obtain the current context window\n  x = torch.stack([data[i:i+block_size] for i in ix])\n   #stack the inputs\n  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #stack the targets\n  return x,y\n\nxb,yb = get_batch('train',batch_size,block_size)\nprint(f'input shape: {xb.shape} ')\nprint(xb)\nprint(f'target shape {yb.shape}')\nprint(yb)\nprint('--------')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:01.140937Z","iopub.execute_input":"2025-05-31T06:14:01.141563Z","iopub.status.idle":"2025-05-31T06:14:01.152410Z","shell.execute_reply.started":"2025-05-31T06:14:01.141526Z","shell.execute_reply":"2025-05-31T06:14:01.151622Z"},"id":"NqIv5C351auc","outputId":"f4227921-01c7-4f30-8ed5-0531cf522f7d","trusted":true},"outputs":[{"name":"stdout","text":"input shape: torch.Size([4, 8]) \ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntarget shape torch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n--------\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 4.Define some baseline model to test our idea","metadata":{"id":"dWvjbaoW249-"}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:44.517431Z","iopub.execute_input":"2025-05-31T06:10:44.518169Z","iopub.status.idle":"2025-05-31T06:10:44.582657Z","shell.execute_reply.started":"2025-05-31T06:10:44.518145Z","shell.execute_reply":"2025-05-31T06:10:44.581925Z"},"id":"vDnRtBy2YLZI","outputId":"50fd737a-9c63-42d3-d06c-3506ca3b90da","trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### 4.1 Bigram model implementation","metadata":{"id":"mfG8ECtJYAIv"}},{"cell_type":"markdown","source":"In order to try the get_batch() function, and start obtaining some meaninful results, it's time to implement some first iteration of a possible model.","metadata":{"id":"ahd4T7Nk3Y4c"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n  #define the constructor\n  def __init__(self,vocabulary_size,n_embedding_dimension):\n    super().__init__()\n    #in our case, we are gonna obtain a table of 65x65 --> convert the token into a embedding representation\n    self.token_embedding_table = nn.Embedding(vocabulary_size,\n                                              n_embedding_dimension)\n    #encoding of positions of the sequence (block size->max length of sequence)\n    self.position_embedding_table = nn.Embedding(block_size,\n                                                 n_embedding_dimension)\n    #add the final linear layer\n    self.lm_head = nn.Linear(n_embedding_dimension,vocabulary_size)\n\n  def forward(self,idx,targets=None):\n    idx = idx.to(device)\n    B,T = idx.shape\n    #obtain the embeddings representation of the table in the position of interest\n    token_embeddings = self.token_embedding_table(idx) #size -> (B,T,C)\n    indices = torch.arange(T,device=device)\n    position_embeddings = self.position_embedding_table(indices % self.position_embedding_table.weight.size(0)) #size -> (T,C)\n    x = token_embeddings + position_embeddings #add the positional information to the embeddings\n    logits = self.lm_head(x) #size -> (batch_size,context_window,vocab_size)\n\n    if targets is None:\n      loss = None\n    else:\n      #define a loss function\n      B,T,C = logits.shape\n      #simplify the dimension of the loss\n      logits = logits.view(B*T,C)\n      target = targets.view(B*T)\n      #define a loss function\n      loss = F.cross_entropy(logits,target)\n    return logits,loss\n\n  #generate tokens\n  def generate(self,idx,max_new_tokens):\n    \"\"\"\n    Generates a sequence of tokens autoregressively based on the given input context.\n\n    Args:\n        idx (Tensor): The current token sequence (shape: [B, T]), where B is the batch size and T is the sequence length.\n        max_new_tokens (int): The number of new tokens to generate.\n\n    Returns:\n        Tensor: The generated sequence with new tokens appended (shape: [B, T + max_new_tokens]).\n    \"\"\"\n    #idx --> current context (B,T)\n    for _ in range(max_new_tokens):\n      logits,_ = self(idx)\n      # Extract the logits for the last predicted token in the sequence\n      logits = logits[:,-1,:]\n      probs = F.softmax(logits,dim=-1)\n      #return 1 element for each batch -> (B,1)\n      # Sample the next token from the probability distribution\n      idx_next = torch.multinomial(probs,num_samples=1)\n      #append element to the current sequence: (B,T+1)\n      idx = torch.cat((idx,idx_next),dim=1)\n    return idx\n\nn_embedding_dimension = 32\nblock_size = 8\nbaseline_model = BigramLanguageModel(vocab_size,n_embedding_dimension).to(device)\nxb,yb = xb.to(device),yb.to(device)\nlogits,loss = baseline_model(xb,yb)\nprint(logits.shape)\nprint(loss)\n\n#try to generate some data\nprint(decode(baseline_model.generate(idx=torch.zeros((1,1),dtype=torch.long,device=device),\n                                     max_new_tokens=100)[0].tolist()))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:10:46.537255Z","iopub.execute_input":"2025-05-31T06:10:46.537572Z","iopub.status.idle":"2025-05-31T06:10:47.532821Z","shell.execute_reply.started":"2025-05-31T06:10:46.537547Z","shell.execute_reply":"2025-05-31T06:10:47.532148Z"},"id":"rvEhg7xE28ER","outputId":"3e65b3f7-4d2f-4fd6-ad3b-cb8f46e76846","trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.6272, device='cuda:0', grad_fn=<NllLossBackward0>)\n\n?YCnx.DkRZkNdc'wf,ZT,OLlT-ebtK\nb:xPT&kMBbUAUG:.XSKgO-33mMGd?KL3auhX:YVXhthXNNuyq&BMWG.tbfF dXENDZaAe\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"#define some hyper-parameters\noptimizer = torch.optim.AdamW(baseline_model.parameters(),lr=1e-3)\nbatch_size  = 32\nepochs = 1000","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:11:17.398274Z","iopub.execute_input":"2025-05-31T06:11:17.398836Z","iopub.status.idle":"2025-05-31T06:11:17.402730Z","shell.execute_reply.started":"2025-05-31T06:11:17.398814Z","shell.execute_reply":"2025-05-31T06:11:17.402092Z"},"id":"JvulGzD1WrVz","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"#define some training loop to test our ideas\nfor epoch in range(epochs):\n  #obtain a batch from our data\n  xb,yb = get_batch('train',batch_size,block_size)\n  xb,yb = xb.to(device),yb.to(device)\n  #evaluate the loss\n  logits, loss =  baseline_model(xb,yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\nprint(loss.item())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:06.419129Z","iopub.execute_input":"2025-05-31T06:14:06.419404Z","iopub.status.idle":"2025-05-31T06:14:07.888748Z","shell.execute_reply.started":"2025-05-31T06:14:06.419388Z","shell.execute_reply":"2025-05-31T06:14:07.888020Z"},"id":"qaucP76LW6N-","outputId":"1c28065e-2fe0-4870-ddff-4a0e15a33c40","trusted":true},"outputs":[{"name":"stdout","text":"2.7903501987457275\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#after a fast training, let's try again the text generator.\nprint(decode(baseline_model.generate(idx=torch.zeros((1,1),dtype=torch.long,device=device),\n                                     max_new_tokens=100)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:11.037183Z","iopub.execute_input":"2025-05-31T06:14:11.037445Z","iopub.status.idle":"2025-05-31T06:14:11.087834Z","shell.execute_reply.started":"2025-05-31T06:14:11.037426Z","shell.execute_reply":"2025-05-31T06:14:11.086975Z"},"id":"3AsGAZCpXukR","outputId":"6ec6f73d-cc52-4387-c78c-dd535d59ca32","trusted":true},"outputs":[{"name":"stdout","text":"\nWqf thekRbridcowi, h O le, b t\n\nHiret bobe aleraS\ngO-and mealatanhe ar bthae uwqhe, vethar dthahoate\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 5.Self-attention\n\nSelf-attention is a key mechanism in deep learning models like Transformers, allowing them to focus on different parts of an input sequence when making predictions. It helps the model capture relationships between words/tokens, regardless of their distance in the sequence.\n\nWhy It Matters?\n\n- Helps models capture long-range dependencies in text.\n- Enables parallel processing (unlike RNNs).\n- Forms the foundation of Transformer models (e.g., GPT, BERT).","metadata":{"id":"jC_Y0X6kZJXi"}},{"cell_type":"markdown","source":"### Mathematical trick in self-attention","metadata":{"id":"6PjuCqnMZMSS"}},{"cell_type":"code","source":"#simple example\ntorch.manual_seed(1337) #for reproducibility\nB,T,C = 4,8,2 #batch size , context window, vocabulary size\nx = torch.rand(B,T,C)\nx[0],x.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:13.526907Z","iopub.execute_input":"2025-05-31T06:14:13.527607Z","iopub.status.idle":"2025-05-31T06:14:13.541989Z","shell.execute_reply.started":"2025-05-31T06:14:13.527582Z","shell.execute_reply":"2025-05-31T06:14:13.541387Z"},"id":"asky7IPQZWoL","outputId":"b6407101-d33e-4645-de1c-dec31c1e2f36","trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(tensor([[0.0783, 0.4956],\n         [0.6231, 0.4224],\n         [0.2004, 0.0287],\n         [0.5851, 0.6967],\n         [0.1761, 0.2595],\n         [0.7086, 0.5809],\n         [0.0574, 0.7669],\n         [0.8778, 0.2434]]),\n torch.Size([4, 8, 2]))"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"'''\n  Since we wanna express in some way the influence of the previous tokens respect\n  the current token, what about doing an average of the previous tokens?\n'''\n#APPROACH 1\n\nxbow = torch.zeros((B,T,C)) #C--> number of channels/features\nfor b in range(B):\n  for t in range(T):\n    xprev = x[b,:t+1]  # (t,C)\n    xbow[b,t] = torch.mean(xprev,0) #average the previous elements","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:14:18.858473Z","iopub.execute_input":"2025-05-31T06:14:18.859302Z","iopub.status.idle":"2025-05-31T06:14:18.865057Z","shell.execute_reply.started":"2025-05-31T06:14:18.859278Z","shell.execute_reply":"2025-05-31T06:14:18.864317Z"},"id":"_wQb-S8iaSK0","trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"Now, let's try to use some mathematical trick to compute the same operation","metadata":{"id":"LpcpgZTfwhkW"}},{"cell_type":"code","source":"torch.manual_seed(17)\na = torch.tril(torch.ones(3,3))\na = a/torch.sum(a,1,keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a@b\nprint('a=')\nprint(a)\nprint('---')\nprint('b=')\nprint(b)\nprint('---')\nprint('c=')\nprint(c)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:21.591782Z","iopub.execute_input":"2025-05-31T06:14:21.592357Z","iopub.status.idle":"2025-05-31T06:14:21.611007Z","shell.execute_reply.started":"2025-05-31T06:14:21.592335Z","shell.execute_reply":"2025-05-31T06:14:21.610363Z"},"id":"1ErFQxrawpu_","outputId":"20e6d66a-641d-4297-b5d0-726b06a9527f","trusted":true},"outputs":[{"name":"stdout","text":"a=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n---\nb=\ntensor([[9., 5.],\n        [1., 2.],\n        [0., 9.]])\n---\nc=\ntensor([[9.0000, 5.0000],\n        [5.0000, 3.5000],\n        [3.3333, 5.3333]])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"We can see clearly, that with that trick, we are computing the average of the elements step by step. Firt the 2 ones, then the 4 ones, and then, all at once. Let's implement this idea in the previous code.","metadata":{"id":"QTeHc9-hxDva"}},{"cell_type":"code","source":"#APPROACH 2\nweights = torch.tril(torch.ones(T,T))\nweights = weights/weights.sum(1,keepdim=True)\nprint(weights)\nxbow2 = weights@x #(T,T) @ (B,T,C) --> (B,T,C)\ntorch.allclose(xbow,xbow2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:24.055323Z","iopub.execute_input":"2025-05-31T06:14:24.055634Z","iopub.status.idle":"2025-05-31T06:14:24.070431Z","shell.execute_reply.started":"2025-05-31T06:14:24.055613Z","shell.execute_reply":"2025-05-31T06:14:24.069903Z"},"id":"HSekXyKfxU7S","outputId":"195a5c73-027d-4aea-901e-f681d5ec270a","trusted":true},"outputs":[{"name":"stdout","text":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"Now, it's time to add the use of **softmax**, to simulate the self-attention mechanism that we know.\nSoftmax formula:\n\n\n![softmax](https://miro.medium.com/v2/resize:fit:600/0*fbg5QEc2Lv8IIKcq.png)","metadata":{"id":"2HPrrgltyWop"}},{"cell_type":"code","source":"#APPROACH 3\ntril = torch.tril(torch.ones(T,T))\nprint(tril)\nweights = torch.zeros((T,T))\nweights = weights.masked_fill(tril==0,float('-inf'))\nprint(weights)\nweights = F.softmax(weights,dim=-1)\nprint(weights)\nxbow3 = weights@x\ntorch.allclose(xbow,xbow3)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:28.870959Z","iopub.execute_input":"2025-05-31T06:14:28.871221Z","iopub.status.idle":"2025-05-31T06:14:28.883686Z","shell.execute_reply.started":"2025-05-31T06:14:28.871203Z","shell.execute_reply":"2025-05-31T06:14:28.883000Z"},"id":"cv_xdckIyhdg","outputId":"2b0d3450-3b3e-4a35-c6e8-ab1109b3e7ad","trusted":true},"outputs":[{"name":"stdout","text":"tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1., 1., 1.]])\ntensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"#APPROACH 4\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 #4 batches, 8 tokens per context window, 32 elements to represent the embedding\nx = torch.rand(B,T,C)\n\ntril = torch.tril((torch.ones(T,T)))\nweights = torch.zeros((T,T))\nweights = weights.masked_fill(tril==0,\n                              float('-inf'))\nweights = F.softmax(weights,dim=-1)\nout = weights@x\nout.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:30.711720Z","iopub.execute_input":"2025-05-31T06:14:30.712218Z","iopub.status.idle":"2025-05-31T06:14:30.721099Z","shell.execute_reply.started":"2025-05-31T06:14:30.712197Z","shell.execute_reply":"2025-05-31T06:14:30.720451Z"},"id":"-DhGHLJLXa_8","outputId":"6734c391-ea71-4812-d4c0-ef8c8ac52d6a","trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 32])"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"In the self-attention mechanism, for each word vector in the input sequence, we create three distinct vectors:\n\n- Query vector (Q): This represents the word we are currently focusing on. It is used to compare against all the other word vectors in the sequence.\n\n- Key vector (K): This represents every word in the sequence from the perspective of the word we are focusing on. It is used to match with the query to determine how much attention a word should pay to other words.\n\n- Value vector (V): This contains the actual information that will be used in the final output. The value vector is weighted based on the similarity between the query and the key vectors.\n\n\n**Overall Process:**\n\n- For each word in the sequence, compute its query (Q), key (K), and value (V) using learned linear transformations.\n- Compute the attention scores by taking the dot product of the query vector with the key vectors of all words, followed by a softmax function to normalize these scores.\n- Use the attention scores to compute a weighted sum of the value vectors.\n\nThe resulting weighted sum represents how much information from the other words (and itself) contributes to the representation of the current word.\n\nThis mechanism allows the model to dynamically focus on the most relevant parts of the sequence for each word, capturing dependencies and context efficiently.\n\n\n**The Role of the Head in Multi-Head Attention**\n\nInstead of computing self-attention once, transformers use multiple attention heads in parallel to capture different types of relationships between words. Each head learns a different attention pattern, allowing the model to focus on various parts of the sequence simultaneously.\n\nEach head independently computes its own Q, K, and V matrices using different learned weight matrices.\n\nThese separate attention mechanisms allow the model to capture different semantic relationships (e.g., long-range dependencies, syntactic structure, coreference resolution).\n\nAfter computing the attention outputs from multiple heads, they are concatenated and passed through a linear transformation to merge the information.\n\nSo, let's try to use this concepts in order to define the self-attention mechanism in a proper way.","metadata":{"id":"afDVP4wtZGmv"}},{"cell_type":"code","source":"#APPROACH 4\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 #4 batches, 8 tokens per context window, 32 elements to represent the embedding\nx = torch.rand(B,T,C)\n\n#Implementation of a single Head that performs self-attention\nhead_size = 16\n\n# Create a linear transformation for the key vectors\n# The input dimension is C (size of word embeddings), and the output dimension is head_size (size of key vector for each attention head)\n# No bias term is added to this transformation\nkey = nn.Linear(C, head_size, bias=False)\n\n# Create a linear transformation for the query vectors\n# The input dimension is C (size of word embeddings), and the output dimension is head_size (size of query vector for each attention head)\n# No bias term is added to this transformation\nquery = nn.Linear(C, head_size, bias=False)\n\n#definition of value vector\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x) # (B,T,16)\nq = query(x) # (B,T,16)\nv = value(x)\n\n#we want to obtain the relationship between the key vectors and query vectors for every word in the context window\nscores = q @ k.transpose(-2,-1)*(head_size**-0.5) # (B,T,16) @ (B,16,T) --> (B,T,T)\n\ntril = torch.tril((torch.ones(T,T)))\nscores = scores.masked_fill(tril==0,\n                              float('-inf'))\nscores = F.softmax(scores,dim=-1)\nout = scores@v\nout.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:34.126490Z","iopub.execute_input":"2025-05-31T06:14:34.127227Z","iopub.status.idle":"2025-05-31T06:14:34.139657Z","shell.execute_reply.started":"2025-05-31T06:14:34.127204Z","shell.execute_reply":"2025-05-31T06:14:34.139045Z"},"id":"gdX2xUMgZvFC","outputId":"8638dde8-955c-4170-abce-ecae0663fda8","trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 16])"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"weights[0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:40.786079Z","iopub.execute_input":"2025-05-31T06:14:40.786790Z","iopub.status.idle":"2025-05-31T06:14:40.792242Z","shell.execute_reply.started":"2025-05-31T06:14:40.786766Z","shell.execute_reply":"2025-05-31T06:14:40.791542Z"},"id":"VyjHAFmrbQA-","outputId":"b8819441-7f32-4824-9a64-3ca77caca281","trusted":true},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([1., 0., 0., 0., 0., 0., 0., 0.])"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"Let's see at the attention score output and analyse the result:\n- Each row corresponds to a token in the sequence.\n- Each column represents how much attention is paid to other tokens (or itself) by that specific token.\n- The values in the tensor are normalized (typically using softmax), indicating the strength of attention, ranging between 0 and 1.\n\nAdditional notes:\n\n-  Attention mechanism is a communication mechanism.\n- There is no notion of what space is. Attention simply acts over a set of vectors.\n- Each sample across batch dimension is processed independently and never 'communicate' with each other sample.\n- In the case that we are doing an **encoder** attention block, the part of the **tril masking* is not neccesary because we want to allow the tokens to communicate each other. In the case of a **decoder** block , it's necessary since we want to make the model to generate new tokens.\n- In **self-attention**, Q,K,V are generated by the same data. In the case of **cross-attention**, keys and values come from an external source.\n- Scaled attention further adjusts the attention weights by dividing them by 1/sqrt(head_size). This scaling ensures that when the input query (Q) and key (K) vectors have unit variance, the resulting attention weights also maintain unit variance. Consequently, the softmax function produces more balanced and diffuse probabilities, preventing it from becoming overly concentrated or saturating excessively.\n\n","metadata":{"id":"-_0ETD1XdL-E"}},{"cell_type":"markdown","source":"## 6. Create the self-attention head in the previously defined model and finish the decoder implementation\n\n![Image](https://i.sstatic.net/nV7Ee.jpg)","metadata":{"id":"4pMp1HoLnASj"}},{"cell_type":"code","source":"'''\n  Demo of the data that we are going to get to try the model\n'''\n\ntorch.manual_seed(1337) #for reproducibility\nbatch_size = 4 #groups to be process in parallel\nblock_size = 16 #context window lenght\n\ndef get_batch(split,batch_size,block_size):\n  data = train_data if split=='train' else validation_data\n  #define the index to obtain the information\n  ix = torch.randint(len(data) - block_size, (batch_size,))\n  #obtain the current context window\n  x = torch.stack([data[i:i+block_size] for i in ix])\n   #stack the inputs\n  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #stack the targets\n  return x,y\n\nxb,yb = get_batch('train',batch_size,block_size)\nprint(f'input shape: {xb.shape} ')\nprint(xb)\nprint(f'target shape {yb.shape}')\nprint(yb)\nprint('--------')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:14:45.254685Z","iopub.execute_input":"2025-05-31T06:14:45.255167Z","iopub.status.idle":"2025-05-31T06:14:45.263788Z","shell.execute_reply.started":"2025-05-31T06:14:45.255144Z","shell.execute_reply":"2025-05-31T06:14:45.263152Z"},"id":"Uu53X_VIuuKy","outputId":"cda3f824-ae88-4427-ce77-beed4888627a","trusted":true},"outputs":[{"name":"stdout","text":"input shape: torch.Size([4, 16]) \ntensor([[21, 27, 24, 13, 26, 33, 31, 10,  0, 32, 59, 57, 46,  6,  1, 58],\n        [53, 59, 57,  1, 51, 43, 52,  0, 13, 56, 43,  1, 39, 58,  1, 58],\n        [50,  6,  1, 57, 47, 56,  8,  1, 18, 39, 56, 43,  1, 63, 53, 59],\n        [58, 46,  1, 57, 53,  6,  1, 46, 53, 50, 63,  1, 57, 47, 56, 11]])\ntarget shape torch.Size([4, 16])\ntensor([[27, 24, 13, 26, 33, 31, 10,  0, 32, 59, 57, 46,  6,  1, 58, 59],\n        [59, 57,  1, 51, 43, 52,  0, 13, 56, 43,  1, 39, 58,  1, 58, 46],\n        [ 6,  1, 57, 47, 56,  8,  1, 18, 39, 56, 43,  1, 63, 53, 59,  1],\n        [46,  1, 57, 53,  6,  1, 46, 53, 50, 63,  1, 57, 47, 56, 11,  1]])\n--------\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass Head(nn.Module):  # Represents a single attention head in a multi-head attention mechanism.\n    def __init__(self, head_size):\n        super().__init__()\n        # Linear transformations for Key (K), Query (Q), and Value (V) vectors.\n        self.key = nn.Linear(n_embedding_dimension, head_size, bias=False)\n        self.query = nn.Linear(n_embedding_dimension, head_size, bias=False)\n        self.value = nn.Linear(n_embedding_dimension, head_size, bias=False)\n        # Register a lower triangular matrix to apply masking for causal attention.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # B: batch size, T: sequence length, C: embedding dimension\n        B, T, C = x.shape\n\n        # Compute the key (K), query (Q), and value (V) vectors.\n        k = self.key(x)  # Shape: (B, T, head_size)\n        q = self.query(x)  # Shape: (B, T, head_size)\n        v = self.value(x)  # Shape: (B, T, head_size)\n\n        # Calculate the attention scores by taking the dot product of Q and K (scaled by sqrt(C)).\n        attention_scores = q @ k.transpose(-2, -1) * C**-0.5  # Shape: (B, T, T)\n\n        # Apply causal masking to prevent attention to future tokens.\n        attention_scores = attention_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n\n        # Normalize the scores using softmax to get attention weights.\n        attention_scores = F.softmax(attention_scores, dim=-1)  # Shape: (B, T, T)\n        attention_scores = self.dropout(attention_scores)\n\n        # Compute the output by applying attention weights to the value (V) vectors.\n        out = attention_scores @ v  # Shape: (B, T, head_size)\n\n        return out\n\nclass MultiHeadAttention(nn.Module):\n  '''\n  Class to define the multihead implementation\n  '''\n  def __init__(self,num_heads,head_size):\n    super().__init__()\n    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    self.proj = nn.Linear(n_embedding_dimension,n_embedding_dimension)\n    self.dropout = nn.Dropout(0.5)\n\n  def forward(self,x):\n    return self.proj(torch.cat([h(x) for h in self.heads],dim=-1))\n\nclass FeedForward(nn.Module):\n  '''\n  Class to define the feedforward implementation\n  '''\n  def __init__(self,n_embedding_dimension):\n    '''\n      Paper 'Attention is all you need' reference.\n      The dimensionality of input and output is d_model=512,\n      and the inner-layer has dimensionality dff=2048. So we have 4 as a factor\n    '''\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embedding_dimension,n_embedding_dimension*4),\n        nn.GELU(),\n        nn.Linear(4*n_embedding_dimension,n_embedding_dimension), #reproject in the same embedding space\n        nn.Dropout(0.5)\n    )\n  def forward(self,x):\n    return self.net(x)\n\n# Layer Normalization (LayerNorm) is used in Transformers to stabilize training,\n# normalize activations, and improve gradient flow. Unlike BatchNorm, it normalizes\n# across features (not batch) and helps prevent internal covariate shift. It is\n# usually applied before or after residual connections to improve convergence.\n\n#time to replicate the mechanism N times in a block form\nclass Block(nn.Module):\n  '''\n  Class to define the block implementation --> communication followed by computation\n  '''\n  def __init__(self,n_embedding_dimension,number_of_heads):\n    super().__init__()\n    head_size = n_embedding_dimension//number_of_heads\n    self.sa = MultiHeadAttention(number_of_heads,head_size)\n    self.ffwd = FeedForward(n_embedding_dimension)\n    self.ln1 = nn.LayerNorm(n_embedding_dimension)\n    self.ln2 = nn.LayerNorm(n_embedding_dimension)\n\n  def forward(self,x):\n    x = x + self.sa(self.ln1(x)) #add residual connections\n    x = x + self.ffwd(self.ln2(x)) #add residual connections\n    return x\n\nclass BigramLanguageModel(nn.Module):\n  #define the constructor\n  def __init__(self,vocabulary_size,n_embedding_dimension,number_of_heads):\n    super().__init__()\n    #in our case, we are gonna obtain a table of 65x65 --> convert the token into a embedding representation\n    self.token_embedding_table = nn.Embedding(vocabulary_size,\n                                              n_embedding_dimension)\n    #encoding of positions of the sequence (block size->max length of sequence)\n    self.position_embedding_table = nn.Embedding(block_size,\n                                                 n_embedding_dimension)\n    self.blocks = nn.Sequential(*[Block(n_embedding_dimension,number_of_heads) for _ in range(number_layers)])\n    self.ln_f = nn.LayerNorm(n_embedding_dimension)\n    #add the final linear layer\n    self.lm_head = nn.Linear(n_embedding_dimension,vocabulary_size)\n\n  def forward(self,idx,targets=None):\n    idx = idx.to(device)\n    B,T = idx.shape\n    #obtain the emebddings representation of the table in the position of interest\n    token_embeddings = self.token_embedding_table(idx) #size -> (B,T,C)\n    indices = torch.arange(T,device=device)\n    #obtain the position information of the tokens\n    position_embeddings = self.position_embedding_table(indices) #size -> (T,C)\n    x = token_embeddings + position_embeddings #add the positional information to the embeddings\n    x = self.blocks(x) #apply the attention in blocks\n    x = self.ln_f(x) #apply layer norm \n    logits = self.lm_head(x) #size -> (batch_size,context_window,vocab_size)\n\n    if targets is None:\n      loss = None\n    else:\n      #define a loss function\n      B,T,C = logits.shape\n      #simplify the dimension of the loss\n      logits = logits.view(B*T,C)\n      target = targets.view(B*T)\n      #define a loss function\n      loss = F.cross_entropy(logits,target)\n    return logits,loss\n\n  #generate tokens\n  def generate(self,idx,max_new_tokens):\n    \"\"\"\n    Generates a sequence of tokens autoregressively based on the given input context.\n\n    Args:\n        idx (Tensor): The current token sequence (shape: [B, T]), where B is the batch size and T is the sequence length.\n        max_new_tokens (int): The number of new tokens to generate.\n\n    Returns:\n        Tensor: The generated sequence with new tokens appended (shape: [B, T + max_new_tokens]).\n    \"\"\"\n    #idx --> current context (B,T)\n    for _ in range(max_new_tokens):\n      #crop idx to the last block_size tokens in order to fit the defined table\n      idx_croped = idx[:,-block_size:]\n      #gets the predictions\n      logits,loss = self(idx_croped)\n      # Extract the logits for the last predicted token in the sequence\n      logits = logits[:,-1,:]\n      probs = F.softmax(logits,dim=-1)\n      #return 1 element for each batch -> (B,1)\n      # Sample the next token from the probability distribution\n      idx_next = torch.multinomial(probs,num_samples=1)\n      if idx_next.max() >= vocab_size:\n            print(f\"Warning: idx_next contains invalid index {idx_next.max()}\")\n\n      #append element to the current sequence: (B,T+1)\n      idx = torch.cat((idx,idx_next),dim=1)\n    return idx\n\n'''\n  Depending on your resources, you can change the parameters\n'''\n\n'''\n#SMALL RESOURCES CONFIGURATION\nn_embedding_dimension = 32\nnumber_of_heads = 4\nblock_size = 256\nnumber_layers = 4\nlearning_rate = 1e-3\n'''\nn_embedding_dimension = 384\nnumber_of_heads = 6  \nblock_size = 256  # Reduced from 256\nnumber_layers = 4  \nlearning_rate = 1e-3\nvocab_size = 65  # Adjust based on your dataset\n\n#define the model\nxb, yb = xb.to(device), yb.to(device)\nbaseline_model = BigramLanguageModel(vocab_size,n_embedding_dimension,number_of_heads).to(device)\nlogits,loss = baseline_model(xb,yb)\nprint(logits.shape)\nprint(loss)\n\n#try to generate some data\nprint(decode(baseline_model.generate(idx=torch.zeros((1,1),dtype=torch.long, device=device),\n                                     max_new_tokens=100)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:15:42.308861Z","iopub.execute_input":"2025-05-31T06:15:42.309133Z","iopub.status.idle":"2025-05-31T06:15:43.547653Z","shell.execute_reply.started":"2025-05-31T06:15:42.309113Z","shell.execute_reply":"2025-05-31T06:15:43.546974Z"},"id":"L1ddJoAqnGHq","outputId":"0945a744-0f4b-48a5-d298-07dca1718d95","trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([64, 65])\ntensor(4.2764, device='cuda:0', grad_fn=<NllLossBackward0>)\n\nM.Ebimuvlig&$!BvX;H:XCu!gwltHO$r;:cH.WqDRCjoAtu;mi sT\nKscTCtZYpu,phHym\n'HcPUHans'ftrc& D;qEQqWgnDbER\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"xb.shape","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:15:48.667391Z","iopub.execute_input":"2025-05-31T06:15:48.667944Z","iopub.status.idle":"2025-05-31T06:15:48.672315Z","shell.execute_reply.started":"2025-05-31T06:15:48.667923Z","shell.execute_reply":"2025-05-31T06:15:48.671668Z"},"id":"1_8LrxczuOIU","trusted":true},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 16])"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"#define some hyper-parameters\noptimizer = torch.optim.AdamW(baseline_model.parameters(),learning_rate)\nbatch_size  = 64 #can try 64/128\nepochs = 5000","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:15:52.794980Z","iopub.execute_input":"2025-05-31T06:15:52.795244Z","iopub.status.idle":"2025-05-31T06:15:52.799740Z","shell.execute_reply.started":"2025-05-31T06:15:52.795224Z","shell.execute_reply":"2025-05-31T06:15:52.799186Z"},"id":"UhTFiuoG1STY","trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"#define some training loop to test our ideas\nfor epoch in range(epochs):\n  #obtain a batch from our data\n  xb,yb = get_batch('train',batch_size,block_size)\n  xb,yb = xb.to(device),yb.to(device)\n  #evaluate the loss\n  logits, loss =  baseline_model(xb,yb)\n  optimizer.zero_grad(set_to_none=True)\n  loss.backward()\n  optimizer.step()\nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2025-05-31T06:15:58.946443Z","iopub.execute_input":"2025-05-31T06:15:58.946759Z","iopub.status.idle":"2025-05-31T06:31:41.552340Z","shell.execute_reply.started":"2025-05-31T06:15:58.946739Z","shell.execute_reply":"2025-05-31T06:31:41.551664Z"},"id":"TO8P5qPb1hdC","outputId":"428543f4-c2b8-4517-a585-14fe3a8fbcd9","trusted":true},"outputs":[{"name":"stdout","text":"0.9927419424057007\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"#after a fast training, let's try the text generator feature\nprint(decode(baseline_model.generate(idx=torch.zeros((1,1),dtype=torch.long,device=device),\n                                     max_new_tokens=500)[0].tolist()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:36:52.418105Z","iopub.execute_input":"2025-05-31T06:36:52.418659Z","iopub.status.idle":"2025-05-31T06:36:57.559020Z","shell.execute_reply.started":"2025-05-31T06:36:52.418638Z","shell.execute_reply":"2025-05-31T06:36:57.558345Z"},"id":"vKLIi9AZ1wqv","outputId":"ad48e41c-7e63-49cb-f8b7-680fe464a439","trusted":true},"outputs":[{"name":"stdout","text":"\n\nPOLIXENES:\nA time no more else\nwhat tale or does dear laider you, wether\nYou lilest at haste.\n\nPERDITA:\nYou will desert knowledge, cannot?\n\nVolsce:\nThis is a thing, must not call much: a legihtch.\nDid I fain cold me as hour is arrived with\npieces, which if he feel not, but kneel'd\netern trails were for revenge. I am not pilect\nnor and heir: when you have sean'd the young youth,\nsputting it with degree so?\n\nFLORIZIZEL:\nO, no, adieve me: we ready\nAnd one suffer, Marcius, weapons, madam.\n\nDUCHESS \n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 7.Save the weights of the model","metadata":{"id":"0VSLbp6Achj7"}},{"cell_type":"code","source":"# Save model weights\nmodel_save_path = \"/kaggle/working/baseline_model_2.pth\"\n\n# Save only the state_dict (recommended)\ntorch.save(baseline_model.state_dict(), model_save_path)\n\nprint(f\"Model weights saved at: {model_save_path}\")#print(\"Model weights saved successfully!\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-05-31T06:37:14.708319Z","iopub.execute_input":"2025-05-31T06:37:14.708622Z","iopub.status.idle":"2025-05-31T06:37:14.775558Z","shell.execute_reply.started":"2025-05-31T06:37:14.708601Z","shell.execute_reply":"2025-05-31T06:37:14.774854Z"},"id":"CoR6W2xGaIV3","outputId":"c33273e4-7fbc-4443-906a-aabac22a6ab7","trusted":true},"outputs":[{"name":"stdout","text":"Model weights saved at: /kaggle/working/baseline_model_2.pth\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 8.Conclusions","metadata":{"id":"qmIU7C-wdybB"}},{"cell_type":"markdown","source":"In this notebook, we implemented a Decoder-based model to generate text from a given .txt file. The key components of our implementation included token embedding, positional encoding, multi-head self-attention, and feedforward layers, which together form the foundation of transformer-based text generation.\n\nWe also explored the self-attention mechanism, which allows the model to dynamically weigh different parts of the input sequence to capture long-range dependencies. Unlike traditional sequence models such as RNNs, self-attention enables efficient parallelization and better context modeling, leading to more coherent text generation.\n\nFuture Improvements:\n- Fine-tuning on domain-specific text for specialized text generation.\n- Exploring larger context windows to enhance coherence in long-form text.\n- Since we have learned the basics of the transformer architecture, we can try to implement variations to the architecture, or try to implement variations in other domains like vision transformer.","metadata":{"id":"SRerBi9Bd3JX"}}]}